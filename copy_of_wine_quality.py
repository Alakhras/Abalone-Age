# -*- coding: utf-8 -*-
"""Copy of wine-quality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KSOTcVCPYF4j0NaGEr1aSdIQ37WGfkF9

# Binary Classification using Tensorflow and Keras
Aladdin Alakhras, University of Missouri-St. Louis

### **Problem:**
* Abalones are endangered marine snails found in cold coastal waters worldwide,  Their price is positively correlated with age.
  However, determining abalone age is complex.
A machine learning model to classify abalone age would significantly accelerate this manual process, benefiting abalone researchers and adding value to the field.

* Dataset: [`Abalone.csv`](https://github.com/Alakhras/Abalone-Age/blob/main/Abalone.csv)
  - This project classifies abalone snails as "young" or "old" based on their ring count, using input features such as gender, height, weight, etc.
* Original data at [UCI ML database](https://archive.ics.uci.edu/dataset/1/abalone)

#Phase 1: Exploratory Data Analysis & preparation of the Abalone Data Set

We are going to use physical and biological attributes of abalone to predict whether an abalone is old or young. There are 4177 observations in this data set and 8 features. We have sex as a categorical feature indicating whether an abalone is male, female or infant. We have other 7 numeric features which describe the size and weight of abalones.
The missing values have been removed in the original research so we do not have missing values here.

Step 1: Load the data and Preview dataset shape and the first 5 rows
"""

import os
import shap
import random
import numpy as np
import pandas as pd
import altair as alt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from prettytable import PrettyTable
from IPython.display import display, Markdown
from sklearn.metrics import precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')

BASE_PATH = 'https://raw.githubusercontent.com/Alakhras/Abalone-Age/main/Abalone.csv'
dataset1 = pd.read_csv(BASE_PATH)
print(dataset1.shape)
dataset1.head()

dataset1.describe()

"""The minimum value in height is **zero**. There are some zero values in height feature so checking the 'height' feature."""

dataset1[dataset1.Height == 0]

"""There are only two rows with **zero** values. We can remove these two rows from our dataset and consider the rest of the data for the model.

"""

dataset2 = dataset1[dataset1.Height != 0]
print(dataset2.shape)
dataset2.describe()
dataset2['Height'].describe()

# Calculate the correlation matrix only for numeric columns.
plt.figure(figsize=(8, 6))  # Adjust the figure size if desired
sns.heatmap(dataset1.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')
plt.show()

"""*  Step 2: Prepare the output
*  Step 3: Shuffle the rows
*  Step 4: Split into Training/Validation Set
*   Step 5: Normalize (if needed)
*   Step 6: Review the dimensions of training & validation set

We categorize abalones based on their age, designating those with more than 10 rings as "old" and those with 10 rings or fewer as "young." This threshold is arbitrary, and we observe an imbalanced distribution of the target variable, which could affect our predictive modeling efforts. Addressing this imbalance is crucial for improving model performance and ensuring reliable classifications.
"""

dataset2["Is old"] = np.where(dataset2["Rings"] > 10, "Old", "Young")
dataset2.head()

"""Then check if the data is imbalanced by calculating what percentage of the output labels are 0 and what percentage are 1 which is:

| Target       | Old | Young |
| -------------| --- | ---   |
| Observations | 1447| 2730  |.
| Percentage   | 35% | 65%   |.

The dataset exhibits some imbalance but is sufficiently balanced for our analysis, allowing us to proceed with our modeling efforts.

###Exploratory data analysis on training & validation sets

#### 3.1 Target variable distribution
Exploratory data analysis gives us a basic understanding of our train and validation data, and we may also find the some useful features to make young-old prediction for abalones. First, we plot the distribution of our target variable. The young-old class is derived from rings, so we also include the distribution of rings into our plot. From the figure we can observe a slightly right skewed distribution of `rings`. Since we set the threshold for old abalone as `rings > 11` , we get an unbalanced distribution of old and young abalone.
"""

# Distribution of our target: rings and is_old
alt.Chart(dataset2, title="Distribution of target variables").mark_bar().encode(
    alt.X(alt.repeat(), type="nominal"), alt.Y("count()")
).repeat(["Rings", "Is old"])

"""#### 3.2 Distribution of categorical variable
Sex is the only categorical variable in this data set. It has three categories: male, female and infant, denoted as M, F, I respectively. The distribution of sex is balanced.
"""

# Distribution of categorical variable: sex
alt.Chart(dataset2, title="Distribution of sex").mark_bar().encode(
    alt.Y("Sex"), alt.X("count()")
)

"""#### 3.3 Distribution of continuous variables
We first get a summary table from our data set. However, this table does not contain intuitive information to help us make prediction.
"""

dataset2.describe()

"""Then we plot the distribution of all numeric features within two targeted classes.From the plot we can group the numeric variables into three groups: (length, diameter), (height), and (whole_weight, shucked_weight, viscera_weight, shell_weight).  
The first group is left skewed. The means of two classes are similar and the old abalones have less deviation from mean.The second group has some outliers and the third group is right skewed. In the third group, we can observe a difference in mean weights and the distribution of old abalones are more bell-shaped.
"""

# Distribution of numeric variables: 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight'
alt.Chart(dataset2, title="Distribution of numeric variables").mark_bar(
    opacity=0.5
).encode(
    alt.X(alt.repeat(), type="quantitative", bin=alt.Bin(maxbins=50)),
    alt.Y("count()", stack=None),
    color=alt.Color("Is old:O", scale=alt.Scale(range=['#2ca02c', '#d62728']))  # Custom ordinal color palette
).repeat(
    [
        "Length",
        "Diameter",
        "Height",
        "Whole weight",
        "Shucked weight",
        "Viscera weight",
        "Shell weight",
    ],
    columns=2,
)

"""####3.4 Correlation Analysis with Target Variable
In our analysis, we aim to explore the correlation between the selected predictor variables—length, height, and whole weight—and the target variable, rings. Our focus is also on identifying potential differences between young and old abalones.

Preliminary observations indicate a noticeable distinction in the regression lines when analyzing the relationships between the predictor variables (length and whole weight) and the target variable (rings) across the two classes. This suggests that the age classification of abalones may influence their physical characteristics and, consequently, their correlation with the number of rings.
"""

# Base scatter plot for Length vs. Rings
point_length = (
    alt.Chart(
        dataset2,
        title="A Difference in Correlation Between Length and Rings for Old and Young Abalones",
    )
    .mark_circle(opacity=0.3)
    .encode(
        x=alt.X("Length:Q", title="Length"),
        y=alt.Y("Rings:Q", title="Rings"),
        color=alt.Color("Is old:N", title="Is Old")  # Ensure 'Is old' is treated as nominal
    )
)

# Regression lines for each group
regression_lines = point_length.transform_regression(
    "Length", "Rings", groupby=["Is old"]
).mark_line(color="red")

# Combine scatter plot and regression lines
combined_chart = point_length + regression_lines

# Display the combined chart
combined_chart.display()

# Base scatter plot for Height vs. Rings
point_height = (
    alt.Chart(
        dataset2,
        title="A Difference in Correlation Between Height and Rings for Old and Young Abalones",
    )
    .mark_circle(opacity=0.2)
    .encode(
        x=alt.X("Height:Q", title="Height"),
        y=alt.Y("Rings:Q", title="Rings"),
        color=alt.Color("Is old:N", title="Is Old")  # Ensure 'Is old' is treated as nominal
    )
)

# Regression lines for each group
regression_lines = point_height.transform_regression(
    "Height", "Rings", groupby=["Is old"]
).mark_line(color="red")

# Combine scatter plot and regression lines
combined_chart = point_height + regression_lines

# Display the combined chart
combined_chart.display()

# Base scatter plot for Whole Weight vs. Rings
point_weight = (
    alt.Chart(
        dataset2,
        title="A Difference in Correlation Between Whole Weight and Rings for Old and Young Abalones",
    )
    .mark_circle(opacity=0.3)
    .encode(
        x=alt.X("Whole weight:Q", title="Whole Weight"),  # Ensure whole weight is treated as quantitative
        y=alt.Y("Rings:Q", title="Rings"),                # Ensure rings is treated as quantitative
        color=alt.Color("Is old:N", title="Is Old")       # Treat 'Is old' as nominal
    )
)

# Regression lines for each group
regression_lines = point_weight.transform_regression(
    "Whole weight", "Rings", groupby=["Is old"]
).mark_line(color="red")

# Combine scatter plot and regression lines
combined_chart = point_weight + regression_lines

# Display the combined chart
combined_chart.display()

"""#### 3.5 Scatter Plots Showing the Relationship Between Continuous Features
We expect collinearity among the numeric features. Size measurements—length, diameter, and height—are strongly correlated with each other and also correlate with weight. This can be a concern for models sensitive to highly correlated features.

In the first group of scatter plots, we see that length, diameter, and height are linearly correlated. The relationship between size and weight, however, appears to be more non-linear. In the second group of scatter plots focused on weight, the correlations among weight features exist but are generally weaker than those among size features. The relationship between weight and age (rings) is not very clear, but there seems to be a slight positive correlation, suggesting that older abalones may weigh more.

"""

# Define the features to compare
features = ["Length", "Diameter", "Height", "Whole weight"]

# Create a scatter plot matrix for the specified features
scatter_matrix = alt.Chart(dataset2, title="Scatter Plot Matrix of Numeric Variables").mark_point(
    size=5, opacity=0.1
).encode(
    x=alt.X(alt.repeat("row"), type="quantitative"),
    y=alt.Y(alt.repeat("column"), type="quantitative"),
).properties(
    height=200, width=200
).repeat(
    row=features,  # Rows correspond to features for the y-axis
    column=features  # Columns correspond to features for the x-axis
)

# Display the scatter plot matrix
scatter_matrix.display()

# Define the features to compare
features = ["Shucked weight", "Viscera weight", "Shell weight", "Rings"]

# Create a scatter plot matrix for the specified features
scatter_matrix = alt.Chart(dataset2, title="Scatter Plot Matrix of Numeric Variables").mark_point(
    size=5, opacity=0.1
).encode(
    x=alt.X(alt.repeat("column"), type="quantitative"),  # X-axis uses features for columns
    y=alt.Y(alt.repeat("row"), type="quantitative"),     # Y-axis uses features for rows
).properties(
    height=200, width=200
).repeat(
    column=features,  # Columns correspond to features for the x-axis
    row=features      # Rows correspond to features for the y-axis
)

# Display the scatter plot matrix
scatter_matrix.display()

"""#### 3.6 Correlation Heat Map
A correlation heat map offers a visually intuitive representation of the relationships between all variables in our dataset. By examining the heat map, we can identify that the feature variables exhibit high levels of correlation. This visualization allows us to easily discern patterns and potential multicollinearity among the numeric features, which is critical for informing our modeling decisions and understanding the underlying data structure.
"""

# Calculate the Spearman correlation matrix and reshape it
corr_df = (
    dataset2.drop(["Sex", "Is old"], axis=1)  # Drop non-numeric features
    .corr("spearman")                       # Calculate the Spearman correlation
    .abs()                                   # Take absolute values of correlations
    .stack()                                 # Reshape to long format
    .reset_index(name="corr")               # Reset index and rename the column
)

# Create a correlation heat map using Altair
heatmap = alt.Chart(corr_df).mark_rect().encode(
    x=alt.X("level_0:O", title="Features", sort="ascending"),  # x-axis for features
    y=alt.Y("level_1:O", title="Features", sort="ascending"),  # y-axis for features
    color=alt.Color("corr:Q", title="Correlation", scale=alt.Scale(domain=[0, 1], range=["lightblue", "darkblue"])),  # Color scale
    tooltip=["level_0", "level_1", "corr"]  # Tooltip for feature details
).properties(
    title="Collinearity Heat Map",  # Chart title
    width=400,
    height=400
)

# Display the heat map
heatmap.display()

"""#Convert 'Sex' and 'Is old' columns to numeric representation"""

# Convert 'Sex' column to numeric representation
# Assuming 'Sex' column contains 'M' , 'F' and 'I'
dataset2['Sex'] = dataset2['Sex'].map({'M': 1, 'F': 0, 'I': 2}).astype(float)  # Apply astype(float) to the Series after mapping

#Convert 'Is old' column to numeric representation
dataset2['Is old'] = dataset2['Is old'].map({'Old': 1, 'Young': 0}).astype(float)  # Apply astype(float) to the Series after mapping

dataset = dataset2.to_numpy()

np.set_printoptions(formatter={'float': lambda x: "{0:0.2f}".format(x)})
print(dataset[0:5, :])

"""##Shuffling the dataset for reliability"""

# Shuffle the dataset
np.random.shuffle(dataset)

np.set_printoptions(formatter={'float': lambda x: "{0:0.2f}".format(x)})
print(dataset[0:5, :])

"""##Split into Training/Validation Set"""

index_20percent = int(0.2 * len(dataset[:, 0]))
print(index_20percent)

XVALIDATION = dataset[:index_20percent, :-1]
YVALIDATION = dataset[:index_20percent, -1]

XTRAIN = dataset[index_20percent:, 0:-1]
YTRAIN = dataset[index_20percent:, -1]

"""##Normalize the dataset"""

plt.hist(XTRAIN[:, 8])
plt.ylabel('8th Column (fixed Rings)')
plt.show()

# Histogram for training set output labels
plt.figure(figsize=(10, 4))  # Set figure size
plt.hist(YTRAIN, bins=30, color='blue', alpha=0.7)  # Adjust number of bins and color
plt.title('Distribution of Output Labels in Training Set')  # Title for the histogram
plt.xlabel('Output Labels')  # X-axis label
plt.ylabel('Frequency')      # Y-axis label
plt.grid(axis='y', alpha=0.5)  # Optional grid for better readability
plt.show()

# Histogram for validation set output labels
plt.figure(figsize=(10, 4))  # Set figure size
plt.hist(YVALIDATION, bins=30, color='orange', alpha=0.7)  # Adjust number of bins and color
plt.title('Distribution of Output Labels in Validation Set')  # Title for the histogram
plt.xlabel('Output Labels')  # X-axis label
plt.ylabel('Frequency')      # Y-axis label
plt.grid(axis='y', alpha=0.5)  # Optional grid for better readability
plt.show()

# Normalize features only for the training set
X_min = np.min(XTRAIN, axis=0)
X_max = np.max(XTRAIN, axis=0)
X_TRAIN_normalized = (XTRAIN - X_min) / (X_max - X_min)

X_VALIDATION_normalized = (XVALIDATION - X_min) / (X_max - X_min)  # Use the same min and max to scale test data
print(X_TRAIN_normalized.shape)
print(X_VALIDATION_normalized.shape)

# Print shapes of the datasets
print("Shapes of the datasets:")
print("XTRAIN shape:", XTRAIN.shape)
print("YTRAIN shape:", YTRAIN.shape)
print("X_VALIDATION_normalized shape:", X_VALIDATION_normalized.shape)
print("X_TRAIN_normalized shape:", X_TRAIN_normalized.shape)
print("YVALIDATION shape:", YVALIDATION.shape)

# Print first three rows of each dataset
print("\nFirst three entries of the datasets:")
print("XTRAIN (first 3 rows):")
print(XTRAIN[0:3,])  # Assuming XTRAIN is a NumPy array
print("YTRAIN (first 3 entries):")
print(YTRAIN[0:3])

print("\nX_VALIDATION_normalized (first 3 rows):")
print(X_VALIDATION_normalized[0:3,])  # Assuming X_VALIDATION_normalized is a NumPy array
print("X_TRAIN_normalized (first 3 rows):")
print(X_TRAIN_normalized[0:3,])  # Assuming X_TRAIN_normalized is a NumPy array

print("\nYVALIDATION (first 3 entries):")
print(YVALIDATION[0:3])  # Assuming YVALIDATION is a NumPy array

# Plot histogram for the 8th column of the normalized training set
plt.figure(figsize=(10, 5))  # Set figure size for better visibility
plt.hist(X_TRAIN_normalized[:, 8], bins=30, color='skyblue', alpha=0.7)  # Specify the number of bins and colors
plt.title('Distribution of the 8th Column (Fixed Rings) in Normalized Training Set')  # Title for the histogram
plt.xlabel('Fixed Rings')  # X-axis label
plt.ylabel('Frequency')     # Y-axis label
plt.grid(axis='y', alpha=0.5)  # Optional grid for better readability
plt.show()

"""###Phase 2: Build a model to overfit the entire dataset

*   Step 7: Create a neural network model
*   Step 8: Compile the model
*   Step 9: Train the model
*   Step 10: Check the learning curves
*   Step 11: Evaluate the model on the dataset
*   Step 13: Check what what the model actually predicts
*   Step 14: Is 'accuracy' sufficient to evaluate our model?

we want to determine how big architecture we need to overfit the data. The place to start is to use ‘logistic regression’ model and train for as many epochs as needed to obtain as high accuracy as possible so we will use the dataset without splitting then normalize it.
"""

# Separate features and target
X = dataset[:, :-1]
y = dataset[:, -1]

# Normalize features
X_min = np.min(X, axis=0)
X_max = np.max(X, axis=0)
X_normalized = (X - X_min) / (X_max - X_min)

"""#Model Creation and Training:
 The function create_compile_model initializes a neural network based on specified architecture. The initial model is set up as a logistic regression with one neuron. Training the model produces learning curves that illustrate how loss and accuracy evolve over epochs, providing insights into model performance.

"""

# 1. Model Creation and Training: A function to create and compile a neural network model is defined, allowing for flexibility in architecture.
def create_compile_model(input_shape, layers):
    model = Sequential()  # Initialize a Sequential model
    for i, neurons in enumerate(layers):
        if i == 0:
            model.add(Dense(neurons, input_shape=input_shape, activation='relu'))  # Input layer with ReLU activation
        else:
            model.add(Dense(neurons, activation='relu'))  # Hidden layers with ReLU activation
    model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

    # Compile the model
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# The initial model starts as a logistic regression model (1 neuron)
model = create_compile_model((X_normalized.shape[1],), [1])

# After training, the learning curves for both loss and accuracy are plotted to visualize the model's performance.
history = model.fit(X_normalized, y, epochs=100, verbose=1)

"""#Evaluation:
After training, the model is evaluated on the entire dataset. Predictions are generated, enabling the calculation of accuracy, precision, recall, and F1 score. Displaying the first ten predictions alongside the actual values provides a direct comparison, allowing for assessment of prediction quality.
"""

# Plot learning curves
plt.plot(history.history['loss'], label='Loss')
plt.plot(history.history['accuracy'], label='Accuracy')
plt.title('Learning Curves')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.show()
print('\n')
# 2. Evaluation: The model is evaluated on the entire dataset, and predictions are made,
# followed by calculating additional metrics (precision, recall, and F1 score).
accuracy = model.evaluate(X_normalized, y, verbose=1)[1]
print(f'Final accuracy on entire dataset: {accuracy * 100:.2f}%')
print('\n')
# Check model predictions
predictions = model.predict(X_normalized).flatten()  # Get raw predictions
predictions_binary = np.round(predictions)  # Convert predictions to binary (0 or 1)
print('\n')
# Analyze predictions
print(f'Predictions:         {predictions[:10]}')
print(f'Binary Predictions:  {predictions_binary[:10]}')
print(f'True Values:         {y[:10]}')

# Additional Metrics
precision = precision_score(y, predictions_binary)
recall = recall_score(y, predictions_binary)
f1 = f1_score(y, predictions_binary)

print(f'Precision:           {precision:.2f}')
print(f'Recall:              {recall:.2f}')
print(f'F1 Score:            {f1:.2f}')

"""#Iterative Model Growth:
  The function iteratively_increase_model systematically tests various configurations of the neural network by varying the number of neurons in the hidden layer. The architectures tested include 2, 4, 8, 16, 32, and 64 neurons. Each configuration's accuracy is evaluated:

Architecture Evaluation: After fitting the model for each specified number of neurons, its accuracy is calculated. All tested architectures are logged for later review.
Overfitting Check: The evaluation stops if any architecture achieves an accuracy of 99.5%, which could indicate overfitting.
Summary of Results: After testing all configurations, a summary is printed to show the performance of each tested architecture and highlight the best-performing configuration. This iterative process aims to discover the optimal architecture that can overfit the dataset, which will later guide efforts to simplify the model for better generalization.
The function iteratively_increase_model tests various configurations by changing the number of neurons in the hidden layer. It tracks the accuracy for each architecture tested and identifies the configuration that performs best. The process stops when a model achieves at least 99.5% accuracy, indicating potential overfitting.

The function also records and prints a summary of results, detailing how each architecture performed, which is crucial in determining the minimal architecture needed to potentially overfit the dataset. Insights gained from this iterative growth will guide the process of model simplification for generalization in later phases.
"""

# 3. Iterative Model Growth: A separate function is implemented to test various neuron configurations (2, 4, 8, 16 neurons).
def iteratively_increase_model():
    architectures_tested = []
    best_accuracy = 0
    best_architecture = None

    # Vary the number of neurons in simple iterative architectures
    for neurons in [2, 4, 8, 16, 32]:
        print(f"\nTesting architecture with {neurons} neurons")
        model = create_compile_model((X_normalized.shape[1],), [neurons])
        history = model.fit(X_normalized, y, epochs=500, verbose=0)  # Increase epochs for deeper training

        # Evaluate the model on the entire dataset
        accuracy = model.evaluate(X_normalized, y, verbose=0)[1]
        architectures_tested.append((neurons, accuracy * 100))

        print(f'Architecture with [{neurons}] neurons achieved accuracy: {accuracy * 100:.2f}%')

        # Check if this architecture overfits well
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            best_architecture = neurons

        # Break if exceeding 99% accuracy signifies overfitting
        if accuracy >= 0.995:
            print(f'Overfit achieved with architecture [{neurons}].\n')
            break
# Print summary of results
    print("\nSummary of model architectures tested:")
    for neurons, acc in architectures_tested:
        print(f'Neurons: {neurons}, Accuracy: {acc:.2f}%')

    print(f"\nBest Architecture: [{best_architecture}] neurons with accuracy {best_accuracy * 100:.2f}%")

# Run the architecture testing
iteratively_increase_model()

"""#Phase 3: Model selection & evaluation
*   Step 7: Create a neural network model
*   Step 8: Compile the model
*   Step 9: Train the model
*   Step 10: Check the learning curves
*   Step 11: Evaluate the model on the training data
*   Step 12: Evaluate on validation set
*   Step 13: Check what the model actually predicts
*   Step 14: Is 'accuracy' sufficient to evaluate our model?

#This code implements a machine learning workflow:
 that starts by defining and training a logistic regression model as a baseline for binary classification. It then evaluates multiple neural network architectures to improve performance, using ReLU activation in hidden layers and a sigmoid activation for the output layer. The models are trained and evaluated based on key metrics, including accuracy, precision, recall, and F1 score. Learning curves are plotted to visualize performance over epochs, and the best-performing model is saved using model checkpointing. Finally, a classification report summarizes the model's predictive capabilities, identifying the architecture that achieved the highest validation accuracy.
"""

# Helper function to create neural network models
def create_model(layer_sizes):
    model = keras.Sequential()
    for size in layer_sizes:
        model.add(layers.Dense(size, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))  # For binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Baseline Logistic Regression Implementation
class LogisticRegression:
    def __init__(self):
        self.weights = None
        self.bias = None

    def fit(self, X, y, epochs=1000, learning_rate=0.01):
        self.weights = np.zeros(X.shape[1])
        self.bias = 0
        m = len(y)

        for _ in range(epochs):
            linear_model = np.dot(X, self.weights) + self.bias
            y_predicted = self.sigmoid(linear_model)
            dw = (1 / m) * np.dot(X.T, (y_predicted - y))
            db = (1 / m) * np.sum(y_predicted - y)
            self.weights -= learning_rate * dw
            self.bias -= learning_rate * db

    def predict(self, X):
        linear_model = np.dot(X, self.weights) + self.bias
        y_predicted = self.sigmoid(linear_model)
        return (y_predicted > 0.5).astype(int)

    @staticmethod
    def sigmoid(x):
        return 1 / (1 + np.exp(-x))

# Train the logistic regression model
logistic_model = LogisticRegression()
logistic_model.fit(XTRAIN, YTRAIN)
logistic_acc_train = np.mean(logistic_model.predict(XTRAIN) == YTRAIN)
logistic_acc_val = np.mean(logistic_model.predict(XVALIDATION) == YVALIDATION)

# Define neural network architectures to evaluate
model_architectures = [
    (2, 1),               # Model 1
    (4, 1),               # Model 2
    (8, 1),               # Model 3
    (16, 8, 1),           # Model 4
    (32, 16, 8, 1),       # Model 5
    (64, 32, 16, 8, 1),   # Model 6
]

# Store results in a list
results = []

# Model Checkpointing
checkpoint_path = 'best_model.keras'  # Change file extension to .keras
checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min', verbose=1)

# Training and evaluating neural network models
for arch in model_architectures:
    model = create_model(arch)
    history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALIDATION, YVALIDATION), epochs=50, batch_size=32, verbose=0, callbacks=[checkpoint])

    train_loss, train_acc = model.evaluate(XTRAIN, YTRAIN, verbose=0)
    val_loss, val_acc = model.evaluate(XVALIDATION, YVALIDATION, verbose=0)
    total_params = model.count_params()

    results.append((str(arch), train_acc, val_acc, train_loss, val_loss, total_params))

"""#Plot Learning Curves:
 Learning curves for each architecture are plotted to visualize how both training and validation accuracies evolve over epochs. This helps identify how well each model learns and if any of them are overfitting.

Load the Best Model: The model with the best performance (lowest validation loss) is loaded for final evaluation.

#Predictions and Metrics:
 Predictions are generated for the validation set using the best model. A custom classification report function calculates metrics such as accuracy, precision, recall, and F1 score based on the predictions, providing a detailed assessment of the model's predictive performance.

#Final Results:
The classification metrics are printed, summarizing the efficiency of the best model. Finally, it identifies which architecture performed best based on validation accuracy and prints this information.

This three-part structure effectively encompasses the workflow for logistic regression and neural network model definitions, training, evaluation, and performance reporting. If you have any further questions or need additional explanations, feel free to ask!
"""

# Plot Learning Curves
plt.figure(figsize=(10, 5))
for arch in model_architectures:
    model = create_model(arch) # The indentation of this line and the following line was incorrect. I have corrected it to align with the for loop
    history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALIDATION, YVALIDATION), epochs=50, batch_size=32, verbose=0)


# Plot learning curves
plt.plot(history.history['accuracy'], label='Train Accuracy ' + str(arch))
plt.plot(history.history['val_accuracy'], label='Validation Accuracy ' + str(arch))

# Customize the plot
plt.title('Learning Curves for Various Architectures')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Load the best model
best_model = keras.models.load_model(checkpoint_path)

# Make predictions and evaluate the best model on validation set
y_pred = (best_model.predict(XVALIDATION) > 0.5).astype(int)

# Custom classification report function
def classification_report(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    tn = np.sum((y_true == 0) & (y_pred == 0))
    fp = np.sum((y_true == 0) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == 0))

    accuracy = (tp + tn) / len(y_true)
    precision = tp / (tp + fp) if (tp + fp) else 0.0
    recall = tp / (tp + fn) if (tp + fn) else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0.0

    return {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1_score
    }

# Evaluate the model using additional metrics
report = classification_report(YVALIDATION, y_pred)

# Print classification report
print('\n')
print("Classification Report:")
for key, value in report.items():
    print(f"{key}: {value:.4f}")
# Output results table
# Print logistic regression results
print('\n')
print("Logistic Regression - Training Accuracy   Validation Accuracy")
print(f"                          {logistic_acc_val:.4f}               {logistic_acc_train:.4f}")


results_df = pd.DataFrame(results, columns=['Model Architecture', 'Training Accuracy', 'Validation Accuracy', 'Training Loss', 'Validation Loss', 'Total Parameters'])




# Print results in a single line
print(results_df.to_string(index=False))
# Conclusion about model performance
best_architecture = model_architectures[np.argmax([result[2] for result in results])]
print('\n')
print(f"The best performing model architecture is: {best_architecture} with validation accuracy: {max([result[2] for result in results]):.4f}")

"""![](https://github.com/badriadhikari/DL-2020spring/blob/master/notebooks/neural-network.png?raw=true)

#Discussion on Architecture Size for Overfitting with Output as an Additional Input Feature
When you incorporate the output variable as an extra input feature in your model, you’re essentially providing the model with more data to work with. This additional information can help the model identify more specific patterns and details in the training data.

In simpler terms, if you use a basic model with fewer layers and neurons, it may have trouble grasping the complexities of the data, particularly with this new input. On the other hand, more complex architectures—those with more layers and neurons—offer greater capacity for learning but come with the risk of overfitting.

To achieve overfitting, you might need a model with several hidden layers and a greater number of neurons in each layer (for instance, a structure like 64-32-16-8). Such a deeper network is likely necessary to effectively capture the relationships and dependencies that arise when including the output variable as input. However, it’s crucial to monitor the validation loss to verify whether overfitting truly occurs.
"""

# Define your model architecture
def create_model(layer_sizes):
    model = keras.Sequential()
    for size in layer_sizes:
        model.add(layers.Dense(size, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Custom prediction function
def my_prediction_function(model, data):
    # Extract weights and bias from the model
    weights = [layer.get_weights()[0] for layer in model.layers[:-1]]  # All layers except output layer
    biases = [layer.get_weights()[1] for layer in model.layers[:-1]]   # All biases except output layer
    output_weights = model.layers[-1].get_weights()[0]  # Weights of output layer
    output_bias = model.layers[-1].get_weights()[1]  # Bias of output layer

    # Hidden layers
    z = XTRAIN  # Start with the input data
    for w, b in zip(weights, biases):
        z = np.dot(z, w) + b  # Linear transformation
        z = np.maximum(0, z)  # Apply ReLU activation

    # Output layer
    z = np.dot(z, output_weights) + output_bias  # Linear transformation for output
    result = 1 / (1 + np.exp(-z))  # Sigmoid activation for output
    return result.flatten()  # Ensure it returns a 1D array

# Example usage
# Define neural network architectures to evaluate
layer_sizes = [
    (2, 1),               # Model 1
    (4, 1),               # Model 2
    (8, 1),               # Model 3
    (16, 8, 1),           # Model 4
    (32, 16, 8, 1),       # Model 5
    (64, 32, 16, 8, 1),   # Model 6
]
#Loop through layer_sizes to select the first element of each tuple
updated_layer_sizes = [x[0] for x in layer_sizes] #selecting the first element of each tuple from layer_sizes and store it in a new list named updated_layer_sizes
model = create_model(updated_layer_sizes)

# (Assuming the model has already been trained)
# Now we can predict using both the model and the custom function
# Note: Replace XVALIDATION or any appropriate input data
train_predictions = model.predict(XTRAIN).flatten()
custom_predictions = my_prediction_function(model, XTRAIN)

# Check if predictions are approximately the same
assert np.allclose(train_predictions, custom_predictions), "Predictions do not match!"
print("Predictions from both methods are the same.")

"""#Phase 4: Feature importance and reduction"""

# Sample dataset loading
data = dataset2
# Data preparation
features = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight',
            'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']
target = 'Is old'

# Prepare training and validation data
train_data = data.sample(frac=0.8, random_state=42)
val_data = data.drop(train_data.index)

X_train = train_data[features].to_numpy()
y_train = train_data[target].to_numpy()
X_val = val_data[features].to_numpy()
y_val = val_data[target].to_numpy()

# Function to create a simple model
def create_model(input_shape):
    model = keras.Sequential([
        layers.Dense(10, activation='relu', input_shape=(input_shape,)),
        layers.Dense(1, activation='sigmoid')  # For binary classification
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Step 2: Train models for each feature individually
accuracies = {}
model_checkpoints = []

for i, feature in enumerate(features):
    model = create_model(1)  # Model for one feature only
    checkpoint = ModelCheckpoint(f'model_{feature}.keras', monitor='val_accuracy', save_best_only=True, mode='max')
    model.fit(X_train[:, [i]], y_train, epochs=100, batch_size=5, verbose=0,
              validation_data=(X_val[:, [i]], y_val), callbacks=[checkpoint])
    loss, accuracy = model.evaluate(X_val[:, [i]], y_val, verbose=0)
    accuracies[feature] = accuracy
    model_checkpoints.append(checkpoint)

# Plotting the accuracies of individual features
plt.bar(accuracies.keys(), accuracies.values())
plt.ylim(0, 1)
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy by Feature')
plt.show()

# Step 3: Identify least important features and remove them incrementally
sorted_features = sorted(accuracies.items(), key=lambda x: x[1])
new_accuracies = []
remaining_features = features.copy()

for feature, _ in sorted_features:
    remaining_features.remove(feature)  # Remove the least important feature
    if remaining_features:  # Check if there are remaining features
     # Create and fit a new model with the remaining features
        model = create_model(len(remaining_features))
        checkpoint = ModelCheckpoint('reduced_model.keras', monitor='val_accuracy', save_best_only=True, mode='max')
        model.fit(X_train[:, [features.index(f) for f in remaining_features]], y_train,
                  epochs=100, batch_size=5, verbose=0,
                  validation_data=(X_val[:, [features.index(f) for f in remaining_features]], y_val),
                  callbacks=[checkpoint])

        # Evaluate the model
        loss, accuracy = model.evaluate(X_val[:, [features.index(f) for f in remaining_features]], y_val, verbose=0)
        new_accuracies.append((remaining_features.copy(), accuracy))

# Step 4: Plot the results after feature reduction
x_labels = [' & '.join(feature_set) for feature_set, acc in new_accuracies]
y_values = [acc for _, acc in new_accuracies]

plt.figure(figsize=(10, 6))
plt.bar(x_labels, y_values)
plt.ylabel('Validation Accuracy')
plt.title('Validation Accuracy with Feature Reduction')
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.axhline(y=0.5, color='r', linestyle='--')  # Reference line for 0.5 accuracy
plt.show()

# Step 5: Compare the original model with the reduced feature model
# Original model (using all features)
final_model = create_model(len(features))
checkpoint = ModelCheckpoint('final_model.keras', monitor='val_accuracy', save_best_only=True, mode='max')
final_model.fit(X_train, y_train, epochs=100, batch_size=5, verbose=0,
                validation_data=(X_val, y_val), callbacks=[checkpoint])

# Evaluate the original model
loss, original_accuracy = final_model.evaluate(X_val, y_val, verbose=0)

# Print the accuracies
print(f'Accuracy with all features: {original_accuracy:.4f}')

# Accuracy of the best model after feature reduction
if new_accuracies:
    reduced_accuracy = new_accuracies[-1][1]
    print(f'Best accuracy after feature reduction: {reduced_accuracy:.4f}')
else:
    print('No features remain after removal.')

# Optional: Show the features that were removed and the final accuracy of the reduced model
removed_features = [feature for feature in sorted_features]
print('Removed Features:', ', '.join([feature[0] for feature in removed_features]))

"""#Use model-agnostic methods such as LIME or Shapley values to derive feature importance."""

import pandas as pd
import numpy as np
import shap
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

# Load the Abalone dataset
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'
columns = ['Sex', 'Length', 'Diameter', 'Height', 'WholeWeight',
           'ShuckedWeight', 'VisceraWeight', 'ShellWeight', 'Rings']
abalone_data = pd.read_csv(url, names=columns)

# Filter dataset: Keep only records with Rings > 1
abalone_data = abalone_data[abalone_data['Rings'] > 1]

# Convert 'Sex' into numerical values using one-hot encoding
abalone_data = pd.get_dummies(abalone_data, columns=['Sex'], drop_first=True)

# Separate features and target variable
X = abalone_data.drop('Rings', axis=1)  # Features
y = (abalone_data['Rings'] > 10).astype(int)  # Convert Rings into binary classification

# Split the dataset into training and validation sets
XTRAIN, XVALIDATION, YTRAIN, YVALIDATION = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 1: Reduce datasets to 100 records
XTRAIN = XTRAIN[:100]
YTRAIN = YTRAIN[:100]
XVALIDATION = XVALIDATION[:100]
YVALIDATION = YVALIDATION[:100]  # Ensure this is 100 records

# Convert DataFrame columns to float32
XTRAIN = XTRAIN.astype(np.float32)
XVALIDATION = XVALIDATION.astype(np.float32)

# Step 2: Define the model architecture
final_model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(XTRAIN.shape[1],)),  # Input layer
    layers.Dense(32, activation='relu'),  # Hidden layer
    layers.Dense(1, activation='sigmoid')  # Output layer
])

# Step 3: Compile the model
final_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Adjust loss for your problem type

# Step 4: Train the final model using all features
final_model.fit(XTRAIN, YTRAIN, epochs=100, batch_size=5, verbose=0,
                validation_data=(XVALIDATION, YVALIDATION))

# Step 5: Analyze feature importance using SHAP
K = 100  # Choosing a smaller number of background samples
background_data = shap.sample(XTRAIN, K)  # Create background dataset
explainer = shap.KernelExplainer(final_model.predict, background_data)  # Create SHAP explainer
shap_values = explainer.shap_values(XVALIDATION)  # Compute SHAP values for validation data

# Reshape shap_values if it's a list (binary classification)
if isinstance(shap_values, list):
    shap_values = shap_values[0]  # Select the appropriate SHAP values

# Reshape shap_values to 2D if it's 3D
if shap_values.ndim == 3 and shap_values.shape[2] == 1:  # Reshape if 3D with single output
    shap_values = shap_values.reshape(shap_values.shape[0], shap_values.shape[1])


# Step 6: Visualize SHAP values
shap.summary_plot(shap_values, XVALIDATION, feature_names=X.columns.tolist())  # Use feature names from DataFrame
shap.initjs()  # Initialize JavaScript for visualizations

# For force plot: prepare the first instance to explain
instance_to_explain = XVALIDATION.iloc[0].values.reshape(1, -1)  # Use .iloc to access the row by index
expected_value = explainer.expected_value[0] if isinstance(explainer.expected_value, list) else explainer.expected_value

# Create the force plot
shap.force_plot(expected_value, shap_values[0], instance_to_explain[0], feature_names=X.columns.tolist()) #Also added [0] to shap_values

"""### Step 15: Strategies for Improving Performance

1. **Technical Enhancements**:
   - **Increase Epochs**: One approach to enhance model performance is to increase the number of training epochs to 100 or 150. This will provide the model with additional time to learn from the data.
   - **Add Additional Layers**: Enhancing the complexity of the neural network by adding more layers can increase its learning capacity, allowing it to capture more intricate patterns in the data.

2. **Balance the Dataset**:
   - It is crucial to ensure that the dataset contains an equal number of samples for each class. You can achieve this through:
     - **Oversampling**: Utilize techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate additional samples for the minority class.
     - **Undersampling**: Reduce the number of samples in the majority class to maintain balance within the dataset.

3. **Adjust Training and Validation Set Size**:
   - Modify the number of records in your training and validation sets based on your needs:
     - **Increase Rows**: If your dataset is limited, consider gathering more data to augment it.
     - **Decrease Rows**: If the majority class has a disproportionately large number of records, reducing its size may help in achieving better balance and model performance.

4. **Geographical Location**: Including the locations where abalones are collected can help us understand how different environments affect their characteristics.

5. **Abalone Species**: Adding information about the species will allow us to account for biological differences that may impact classification.

6. **Color**: The color of abalones can vary, and using this feature can help the model better distinguish between different types.

7. **Number of Predators**: Knowing how many natural predators are in the area could provide insights into the abalones' health and behaviors, potentially influencing classification.

8. **Living Environment**: Features such as the type of habitat (e.g., rocky, sandy) and environmental conditions (like pollution and temperature) are crucial for understanding their characteristics.

By implementing these strategies, we can enhance the effectiveness of our classification model.

"""

#@title Convert ipynb to HTML in Colab
!jupyter nbconvert --to html Abalone_age.ipynb